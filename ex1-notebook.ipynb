{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('machine.data',names=['vendor','model','MYCT','MMIN','MMAX','CACH','CHMIN','CHMAX','PRP','ERP'])\n",
    "label = data['ERP']\n",
    "data = data.drop(columns=['ERP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('forestfires.csv')\n",
    "# label = data['area']\n",
    "# data = data.drop(columns=['area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# data = data[['MMIN','MMAX','CACH','CHMIN','CHMAX','PRP']] ## Only numeric attributes\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = pd.get_dummies(data).columns\n",
    "def add_missing_dummy_columns( d, columns ):\n",
    "    missing_cols = set( columns ) - set( d.columns )\n",
    "    for c in missing_cols:\n",
    "        d[c] = 0\n",
    "\n",
    "def fix_columns( d, columns ):  \n",
    "    add_missing_dummy_columns( d, columns )\n",
    "\n",
    "    # make sure we have all the columns we need\n",
    "    assert( set( columns ) - set( d.columns ) == set())\n",
    "\n",
    "    d = d[ columns ]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node(samples, labels, depth, model=None, split_by_feature=None, threshold=None, loss=None, \n",
    "                childrens=[], nominal_value=None):\n",
    "    if model==None:\n",
    "        loss, model = linear_fit(samples,labels)\n",
    "    return {\n",
    "        'samples': samples,\n",
    "        'labels': labels,\n",
    "        'childrens': childrens,\n",
    "        'model': model, \n",
    "        'loss': loss,\n",
    "        'split_by_feature': split_by_feature,\n",
    "        'threshold': threshold,\n",
    "        'nominal_value':nominal_value,\n",
    "        'depth':depth\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def linear_fit(train, label):\n",
    "    reg = LinearRegression().fit(pd.get_dummies(train), label)\n",
    "    mse_score = mean_squared_error(reg.predict(pd.get_dummies(train)), label)\n",
    "    wmse_score = mse_score*len(train)\n",
    "    return wmse_score, reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "try:\n",
    "\n",
    "    getattr(sklearn.linear_model, \"Ridge\")\n",
    "except Exception:\n",
    "    print(\"DA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf=2\n",
    "def numeric_split(feature, train, label, threshold, depth):\n",
    "    mask = train[feature] >= threshold\n",
    "    right_node_samples, right_node_labels = train[mask], label[mask]\n",
    "    left_node_samples, left_node_labels = train[~mask], label[~mask]\n",
    "    if len(left_node_samples)<min_samples_leaf or len(right_node_samples)<min_samples_leaf:\n",
    "        return {'childrens':[],'loss':None}\n",
    "    right_node_wmse, right_node_model = linear_fit(right_node_samples, right_node_labels)\n",
    "    left_node_wmse, left_node_model = linear_fit(left_node_samples, left_node_labels)\n",
    "    wmse = (right_node_wmse + left_node_wmse)/len(train)\n",
    "    mse, model = linear_fit(train,label)\n",
    "    right_node = create_node(right_node_samples, right_node_labels, depth, right_node_model, loss=right_node_wmse)\n",
    "    left_node = create_node(left_node_samples, left_node_labels, depth, left_node_model, loss=left_node_wmse)\n",
    "    \n",
    "    return create_node(train, label, depth, model ,loss=wmse, childrens=[right_node, left_node], split_by_feature=feature, threshold=threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_from_nominal_group(df, depth, nominal_value):\n",
    "    samples = df.drop('Y',axis=1)\n",
    "    labels = df['Y']\n",
    "    node = create_node(samples, labels, depth, nominal_value=nominal_value)\n",
    "    print(node['nominal_value'])\n",
    "    return node\n",
    "\n",
    "def nominal_split(feature, train, label, depth):\n",
    "    df = train.copy()\n",
    "    df['Y'] = label\n",
    "    filtered_nominal_groups = df.groupby(feature).filter(lambda x: len(x)>=min_samples_leaf).groupby(feature)\n",
    "    if len(filtered_nominal_groups)<2:\n",
    "        return None\n",
    "    \n",
    "    nodes = []\n",
    "    wmse = 0\n",
    "    for nominal_value, nominal_group in filtered_nominal_groups:\n",
    "        node = node_from_nominal_group(nominal_group, depth, nominal_value)\n",
    "        nodes.append(node)\n",
    "        wmse += node['loss']\n",
    "    \n",
    "    mse, model = linear_fit(train, label) # We don't use the MSE of the linear_fit, but we train a model in order to use it \n",
    "                                          # at prediction, when the nominal feature value won't be one of the splitted values\n",
    "    print(wmse, len(train), len(filtered_nominal_groups))\n",
    "    wmse = wmse/len(train) \n",
    "    return create_node(train, label, depth, model ,loss=wmse, split_by_feature=feature, childrens=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from operator import itemgetter\n",
    "\n",
    "def split_by_mse(feature, node):\n",
    "    if node['samples'][feature].dtype=='O':\n",
    "        split = nominal_split(feature, node['samples'], node['labels'], node['depth']+1)\n",
    "        if split==None or len(split['childrens'])<2:\n",
    "            return None\n",
    "    else:       \n",
    "        attr_splits = list(set(node['samples'][feature].values))\n",
    "        splits = []\n",
    "        for split in attr_splits:\n",
    "            split_childrens = numeric_split(feature, node['samples'], node['labels'], split, node['depth']+1)\n",
    "            if len(split_childrens['childrens']) > 0 and all(children['loss'] is not None for children in split_childrens['childrens']):\n",
    "                splits.append(split_childrens)\n",
    "        if len(splits)==0:\n",
    "            return None\n",
    "        split = min(splits,key=itemgetter('loss'))\n",
    "        # We filter None due to splits with low number of samples in one of the sides\n",
    "        if node['nominal_value']!=None:\n",
    "            split['nominal_value'] = node['nominal_value'] #Important to keep the nominal_value in case we splitted a categorical node\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node(node):\n",
    "    features = node['samples'].columns\n",
    "    splits = []\n",
    "    for feature in features:\n",
    "        split = split_by_mse(feature, node)\n",
    "        if split!=None:\n",
    "            splits.append(split)\n",
    "    if len(splits)==0:\n",
    "        return node\n",
    "    curr_split = min(splits, key=itemgetter('loss'))\n",
    "    return curr_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_root = create_node(X_train, y_train,depth=0)\n",
    "\n",
    "def expand_tree(node):\n",
    "    node = split_node(node)\n",
    "    if len(node['childrens'])==0:\n",
    "        return node\n",
    "    print(\"Splitted node at depth {d} by feature {f} and threshold {t}\".format(d=node['depth'],f=node['split_by_feature'],t=node['threshold']))\n",
    "    for index, children in enumerate(node['childrens']):\n",
    "        print(\"Node {i} with {n} samples\".format(i=index,n=len(children['samples'])))\n",
    "        node['childrens'][index] = expand_tree(children)\n",
    "        \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amdahl\n",
      "apollo\n",
      "burroughs\n",
      "c.r.d\n",
      "cambex\n",
      "cdc\n",
      "dec\n",
      "dg\n",
      "formation\n",
      "gould\n",
      "harris\n",
      "honeywell\n",
      "hp\n",
      "ibm\n",
      "ipl\n",
      "magnuson\n",
      "nas\n",
      "ncr\n",
      "nixdorf\n",
      "perkin-elmer\n",
      "prime\n",
      "siemens\n",
      "sperry\n",
      "wang\n",
      "4.461805484083457e-22 140 24\n",
      "Splitted node at depth 1 by feature PRP and threshold 84\n",
      "Node 0 with 43 samples\n",
      "amdahl\n",
      "cdc\n",
      "gould\n",
      "honeywell\n",
      "ibm\n",
      "nas\n",
      "ncr\n",
      "siemens\n",
      "sperry\n",
      "8.646380002523284e-23 43 9\n",
      "Splitted node at depth 2 by feature MMAX and threshold 32000\n",
      "Node 0 with 18 samples\n",
      "amdahl\n",
      "honeywell\n",
      "ibm\n",
      "nas\n",
      "sperry\n",
      "8.009515554342808e-23 18 5\n",
      "Splitted node at depth 3 by feature CHMAX and threshold 64\n",
      "Node 0 with 4 samples\n",
      "Splitted node at depth 4 by feature CACH and threshold 128\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 1 with 14 samples\n",
      "amdahl\n",
      "honeywell\n",
      "ibm\n",
      "nas\n",
      "sperry\n",
      "8.077935669463161e-27 14 5\n",
      "Splitted node at depth 4 by feature vendor and threshold None\n",
      "Node 0 with 4 samples\n",
      "Splitted node at depth 5 by feature PRP and threshold 269\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 2 with 2 samples\n",
      "Node 3 with 3 samples\n",
      "Node 4 with 2 samples\n",
      "Node 1 with 25 samples\n",
      "cdc\n",
      "gould\n",
      "ibm\n",
      "nas\n",
      "ncr\n",
      "1.629040935750123e-22 25 5\n",
      "Splitted node at depth 3 by feature CACH and threshold 142\n",
      "Node 0 with 3 samples\n",
      "Node 1 with 22 samples\n",
      "cdc\n",
      "gould\n",
      "ibm\n",
      "nas\n",
      "ncr\n",
      "1.62893592258642e-22 22 5\n",
      "Splitted node at depth 4 by feature CHMAX and threshold 31\n",
      "Node 0 with 4 samples\n",
      "Splitted node at depth 5 by feature CHMAX and threshold 38\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 1 with 18 samples\n",
      "cdc\n",
      "ibm\n",
      "nas\n",
      "1.6289278446507506e-22 18 3\n",
      "Splitted node at depth 5 by feature CHMAX and threshold 10\n",
      "Node 0 with 14 samples\n",
      "cdc\n",
      "ibm\n",
      "nas\n",
      "1.6289116887794117e-22 14 3\n",
      "Splitted node at depth 6 by feature PRP and threshold 109\n",
      "Node 0 with 12 samples\n",
      "cdc\n",
      "ibm\n",
      "nas\n",
      "5.5111716104912415e-25 12 3\n",
      "Splitted node at depth 7 by feature MYCT and threshold 40\n",
      "Node 0 with 9 samples\n",
      "ibm\n",
      "nas\n",
      "1.8175355256292112e-27 9 2\n",
      "Splitted node at depth 8 by feature vendor and threshold None\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 3 samples\n",
      "Node 1 with 3 samples\n",
      "Node 1 with 2 samples\n",
      "Node 1 with 4 samples\n",
      "Splitted node at depth 6 by feature CHMIN and threshold 4\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 1 with 97 samples\n",
      "apollo\n",
      "burroughs\n",
      "c.r.d\n",
      "cambex\n",
      "cdc\n",
      "dec\n",
      "dg\n",
      "formation\n",
      "harris\n",
      "honeywell\n",
      "hp\n",
      "ibm\n",
      "ipl\n",
      "magnuson\n",
      "nas\n",
      "ncr\n",
      "nixdorf\n",
      "perkin-elmer\n",
      "prime\n",
      "siemens\n",
      "sperry\n",
      "wang\n",
      "3.7219088597051514e-25 97 22\n",
      "Splitted node at depth 2 by feature PRP and threshold 42\n",
      "Node 0 with 30 samples\n",
      "c.r.d\n",
      "cambex\n",
      "ibm\n",
      "ipl\n",
      "nas\n",
      "ncr\n",
      "perkin-elmer\n",
      "sperry\n",
      "wang\n",
      "4.0945036424591397e-26 30 9\n",
      "Splitted node at depth 3 by feature MMIN and threshold 2000\n",
      "Node 0 with 20 samples\n",
      "cambex\n",
      "ibm\n",
      "ipl\n",
      "nas\n",
      "ncr\n",
      "4.054113964111824e-26 20 5\n",
      "Splitted node at depth 4 by feature PRP and threshold 52\n",
      "Node 0 with 15 samples\n",
      "cambex\n",
      "ibm\n",
      "ipl\n",
      "nas\n",
      "5.048709793414476e-29 15 4\n",
      "Splitted node at depth 5 by feature vendor and threshold None\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 2 with 2 samples\n",
      "Node 3 with 2 samples\n",
      "Node 1 with 5 samples\n",
      "Splitted node at depth 5 by feature MMAX and threshold 12000\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 3 samples\n",
      "Node 1 with 10 samples\n",
      "c.r.d\n",
      "perkin-elmer\n",
      "wang\n",
      "1.5146129380243427e-28 10 3\n",
      "Splitted node at depth 4 by feature vendor and threshold None\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 2 with 2 samples\n",
      "Node 1 with 67 samples\n",
      "apollo\n",
      "burroughs\n",
      "c.r.d\n",
      "cambex\n",
      "cdc\n",
      "dec\n",
      "dg\n",
      "formation\n",
      "harris\n",
      "honeywell\n",
      "hp\n",
      "ibm\n",
      "magnuson\n",
      "ncr\n",
      "nixdorf\n",
      "prime\n",
      "siemens\n",
      "sperry\n",
      "5.377191474348505e-26 67 18\n",
      "Splitted node at depth 3 by feature MYCT and threshold 320\n",
      "Node 0 with 21 samples\n",
      "apollo\n",
      "c.r.d\n",
      "dec\n",
      "dg\n",
      "formation\n",
      "ibm\n",
      "1.7070949988982695e-27 21 6\n",
      "Splitted node at depth 4 by feature PRP and threshold 12\n",
      "Node 0 with 18 samples\n",
      "apollo\n",
      "c.r.d\n",
      "dec\n",
      "dg\n",
      "formation\n",
      "ibm\n",
      "1.6408306828597046e-28 18 6\n",
      "Splitted node at depth 5 by feature vendor and threshold None\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 2 with 3 samples\n",
      "Node 3 with 2 samples\n",
      "Node 4 with 3 samples\n",
      "Node 5 with 5 samples\n",
      "Splitted node at depth 6 by feature MYCT and threshold 1100\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 3 samples\n",
      "Node 1 with 3 samples\n",
      "Node 1 with 46 samples\n",
      "burroughs\n",
      "cambex\n",
      "cdc\n",
      "dg\n",
      "harris\n",
      "honeywell\n",
      "hp\n",
      "ibm\n",
      "magnuson\n",
      "ncr\n",
      "nixdorf\n",
      "prime\n",
      "siemens\n",
      "sperry\n",
      "2.5874637691249187e-27 46 14\n",
      "Splitted node at depth 4 by feature vendor and threshold None\n",
      "Node 0 with 3 samples\n",
      "Node 1 with 2 samples\n",
      "Node 2 with 2 samples\n",
      "Node 3 with 2 samples\n",
      "Node 4 with 2 samples\n",
      "Node 5 with 4 samples\n",
      "Splitted node at depth 5 by feature MYCT and threshold 300\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 6 with 4 samples\n",
      "Splitted node at depth 5 by feature CHMAX and threshold 24\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 7 with 4 samples\n",
      "Splitted node at depth 5 by feature MMIN and threshold 2000\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n",
      "Node 8 with 3 samples\n",
      "Node 9 with 5 samples\n",
      "Splitted node at depth 5 by feature MMIN and threshold 2000\n",
      "Node 0 with 3 samples\n",
      "Node 1 with 2 samples\n",
      "Node 10 with 2 samples\n",
      "Node 11 with 2 samples\n",
      "Node 12 with 3 samples\n",
      "Node 13 with 4 samples\n",
      "Splitted node at depth 5 by feature MMIN and threshold 512\n",
      "Node 0 with 2 samples\n",
      "Node 1 with 2 samples\n"
     ]
    }
   ],
   "source": [
    "tree = expand_tree(tree_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, sample):\n",
    "    if(len(tree['childrens'])==0): #Leaf\n",
    "        data = fix_columns(pd.get_dummies(sample),pd.get_dummies(tree['samples']).columns)\n",
    "        print(data.shape)\n",
    "        return tree['model'].predict(data)\n",
    "    if tree['threshold']==None: #Categorical\n",
    "        found = False\n",
    "        for index, child in enumerate(tree['childrens']):\n",
    "            if sample[tree['split_by_feature']] == child['nominal_value']:\n",
    "                print(\"Found\")\n",
    "                node = child\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            print(\"Not found\")\n",
    "            return tree['model'].predict(fix_columns(pd.get_dummies(sample),pd.get_dummies(tree['samples']).columns))\n",
    "    else: #Numeric\n",
    "        if(sample[tree['split_by_feature']]>=tree['threshold']):\n",
    "            node = tree['childrens'][0] #right node\n",
    "        else:\n",
    "            node = tree['childrens'][1]\n",
    "    return predict(node, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([102.94052807])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(tree, X_test.iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11)\n",
      "Not found\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Not found\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Not found\n",
      "Found\n",
      "(1, 11)\n",
      "(1, 11)\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "Not found\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Not found\n",
      "(1, 11)\n",
      "(1, 13)\n",
      "Not found\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Not found\n",
      "(1, 12)\n",
      "Not found\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "(1, 12)\n",
      "Not found\n",
      "Not found\n",
      "Not found\n",
      "(1, 10)\n",
      "(1, 12)\n",
      "Found\n",
      "(1, 10)\n",
      "Not found\n",
      "(1, 11)\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "(1, 13)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Not found\n",
      "(1, 12)\n",
      "Not found\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "(1, 12)\n",
      "Found\n",
      "(1, 11)\n",
      "Not found\n",
      "Found\n",
      "(1, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18263.771817108445"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [predict(tree,row.to_frame().T) for index,row in X_test.iterrows()]\n",
    "mean_squared_error(preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "train_dummed = pd.get_dummies(X_train)\n",
    "regressor.fit(train_dummed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4485.289855072464"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = regressor.predict(fix_columns(pd.get_dummies(X_test), pd.get_dummies(X_train).columns))\n",
    "mean_squared_error(y_1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 11)\n",
      "Not found\n",
      "Not found\n",
      "Found\n",
      "(9, 10)\n",
      "Not found\n",
      "Not found\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 11)\n",
      "Not found\n",
      "Found\n",
      "(9, 11)\n",
      "(9, 11)\n",
      "Not found\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 10)\n",
      "Not found\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 11)\n",
      "Found\n",
      "(9, 10)\n",
      "Not found\n",
      "Found\n",
      "(9, 10)\n",
      "Not found\n",
      "Found\n",
      "(9, 10)\n",
      "(9, 10)\n",
      "Not found\n",
      "Found\n",
      "(9, 11)\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 11)\n",
      "Not found\n",
      "(9, 11)\n",
      "(9, 13)\n",
      "Not found\n",
      "Not found\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 10)\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 11)\n",
      "Not found\n",
      "(9, 12)\n",
      "Not found\n",
      "Found\n",
      "(9, 11)\n",
      "Found\n",
      "(9, 10)\n",
      "(9, 12)\n",
      "Not found\n",
      "Not found\n",
      "Not found\n",
      "(9, 10)\n",
      "(9, 12)\n",
      "Found\n",
      "(9, 10)\n",
      "Not found\n",
      "(9, 11)\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 11)\n",
      "(9, 13)\n",
      "Found\n",
      "(9, 10)\n",
      "Found\n",
      "(9, 11)\n",
      "Not found\n",
      "(9, 12)\n",
      "Not found\n",
      "Found\n",
      "(9, 11)\n",
      "Found\n",
      "(9, 10)\n",
      "(9, 12)\n",
      "Found\n",
      "(9, 11)\n",
      "Not found\n",
      "Found\n",
      "(9, 11)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (9!=1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-42690c5b61df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m preds = [predict(tree, X_test.loc[i]) for i, row in\n\u001b[0;32m      2\u001b[0m                 X_test.iterrows()]\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \"\"\"\n\u001b[0;32m    238\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 239\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         raise ValueError(\"y_true and y_pred have different number of output \"\n\u001b[1;32m---> 87\u001b[1;33m                          \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mn_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred have different number of output (9!=1)"
     ]
    }
   ],
   "source": [
    "preds = [predict(tree, X_test.loc[i]) for i, row in\n",
    "                X_test.iterrows()]\n",
    "mean_squared_error(preds,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
