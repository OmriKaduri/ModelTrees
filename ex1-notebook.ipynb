{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('machine.data',names=['vendor','model','MYCT','MMIN','MMAX','CACH','CHMIN','CHMAX','PRP','ERP'])\n",
    "label = data['ERP']\n",
    "data = data.drop(columns=['ERP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('forestfires.csv')\n",
    "# label = data['area']\n",
    "# data = data.drop(columns=['area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# data = data[['MMIN','MMAX','CACH','CHMIN','CHMAX','PRP']] ## Only numeric attributes\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = pd.get_dummies(data).columns\n",
    "def add_missing_dummy_columns( d, columns ):\n",
    "    missing_cols = set( columns ) - set( d.columns )\n",
    "    for c in missing_cols:\n",
    "        d[c] = 0\n",
    "\n",
    "def fix_columns( d, columns ):  \n",
    "    add_missing_dummy_columns( d, columns )\n",
    "\n",
    "    # make sure we have all the columns we need\n",
    "    assert( set( columns ) - set( d.columns ) == set())\n",
    "\n",
    "    d = d[ columns ]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node(samples, labels, depth, model=None, split_by_feature=None, threshold=None, loss=None, \n",
    "                childrens=[], nominal_value=None):\n",
    "    if model==None:\n",
    "        loss, model = linear_fit(samples,labels)\n",
    "    return {\n",
    "        'samples': samples,\n",
    "        'labels': labels,\n",
    "        'childrens': childrens,\n",
    "        'model': model, \n",
    "        'loss': loss,\n",
    "        'split_by_feature': split_by_feature,\n",
    "        'threshold': threshold,\n",
    "        'nominal_value':nominal_value,\n",
    "        'depth':depth\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def linear_fit(train, label):\n",
    "    reg = Ridge().fit(pd.get_dummies(train), label)\n",
    "    mse_score = mean_squared_error(reg.predict(pd.get_dummies(train)), label)\n",
    "    wmse_score = mse_score*len(train)\n",
    "    return wmse_score, reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "try:\n",
    "\n",
    "    getattr(sklearn.linear_model, \"Ridge\")\n",
    "except Exception:\n",
    "    print(\"DA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf=2\n",
    "def numeric_split(feature, train, label, threshold, depth):\n",
    "    mask = train[feature] >= threshold\n",
    "    right_node_samples, right_node_labels = train[mask], label[mask]\n",
    "    left_node_samples, left_node_labels = train[~mask], label[~mask]\n",
    "    if len(left_node_samples)<min_samples_leaf or len(right_node_samples)<min_samples_leaf:\n",
    "        return {'childrens':[],'loss':None}\n",
    "    right_node_wmse, right_node_model = linear_fit(right_node_samples, right_node_labels)\n",
    "    left_node_wmse, left_node_model = linear_fit(left_node_samples, left_node_labels)\n",
    "    wmse = (right_node_wmse + left_node_wmse)/len(train)\n",
    "    mse, model = linear_fit(train,label)\n",
    "    right_node = create_node(right_node_samples, right_node_labels, depth, right_node_model, loss=right_node_wmse)\n",
    "    left_node = create_node(left_node_samples, left_node_labels, depth, left_node_model, loss=left_node_wmse)\n",
    "    \n",
    "    return create_node(train, label, depth, model ,loss=wmse, childrens=[right_node, left_node], split_by_feature=feature, threshold=threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_from_nominal_group(df, depth, nominal_value):\n",
    "    samples = df.drop('Y',axis=1)\n",
    "    labels = df['Y']\n",
    "    node = create_node(samples, labels, depth, nominal_value=nominal_value)\n",
    "    print(node['nominal_value'])\n",
    "    return node\n",
    "\n",
    "def nominal_split(feature, train, label, depth):\n",
    "    df = train.copy()\n",
    "    df['Y'] = label\n",
    "    filtered_nominal_groups = df.groupby(feature).filter(lambda x: len(x)>=min_samples_leaf).groupby(feature)\n",
    "    if len(filtered_nominal_groups)<2:\n",
    "        return None\n",
    "    \n",
    "    nodes = []\n",
    "    wmse = 0\n",
    "    for nominal_value, nominal_group in filtered_nominal_groups:\n",
    "        node = node_from_nominal_group(nominal_group, depth, nominal_value)\n",
    "        nodes.append(node)\n",
    "        wmse += node['loss']\n",
    "    \n",
    "    mse, model = linear_fit(train, label) # We don't use the MSE of the linear_fit, but we train a model in order to use it \n",
    "                                          # at prediction, when the nominal feature value won't be one of the splitted values\n",
    "    print(wmse, len(train), len(filtered_nominal_groups))\n",
    "    wmse = wmse/len(train) \n",
    "    return create_node(train, label, depth, model ,loss=wmse, split_by_feature=feature, childrens=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from operator import itemgetter\n",
    "\n",
    "def split_by_mse(feature, node):\n",
    "    if node['samples'][feature].dtype=='O':\n",
    "        split = nominal_split(feature, node['samples'], node['labels'], node['depth']+1)\n",
    "        if split==None or len(split['childrens'])<2:\n",
    "            return None\n",
    "    else:       \n",
    "        attr_splits = list(set(node['samples'][feature].values))\n",
    "        splits = []\n",
    "        for split in attr_splits:\n",
    "            split_childrens = numeric_split(feature, node['samples'], node['labels'], split, node['depth']+1)\n",
    "            if len(split_childrens['childrens']) > 0 and all(children['loss'] is not None for children in split_childrens['childrens']):\n",
    "                splits.append(split_childrens)\n",
    "        if len(splits)==0:\n",
    "            return None\n",
    "        split = min(splits,key=itemgetter('loss'))\n",
    "        # We filter None due to splits with low number of samples in one of the sides\n",
    "        if node['nominal_value']!=None:\n",
    "            split['nominal_value'] = node['nominal_value'] #Important to keep the nominal_value in case we splitted a categorical node\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node(node):\n",
    "    features = node['samples'].columns\n",
    "    splits = []\n",
    "    for feature in features:\n",
    "        split = split_by_mse(feature, node)\n",
    "        if split!=None:\n",
    "            splits.append(split)\n",
    "    if len(splits)==0:\n",
    "        return node\n",
    "    curr_split = min(splits, key=itemgetter('loss'))\n",
    "    return curr_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_root = create_node(X_train, y_train,depth=0)\n",
    "\n",
    "def expand_tree(node):\n",
    "    node = split_node(node)\n",
    "    if len(node['childrens'])==0:\n",
    "        return node\n",
    "    print(\"Splitted node at depth {d} by feature {f} and threshold {t}\".format(d=node['depth'],f=node['split_by_feature'],t=node['threshold']))\n",
    "    for index, children in enumerate(node['childrens']):\n",
    "        print(\"Node {i} with {n} samples\".format(i=index,n=len(children['samples'])))\n",
    "        node['childrens'][index] = expand_tree(children)\n",
    "        \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amdahl\n",
      "apollo\n",
      "burroughs\n",
      "c.r.d\n",
      "cambex\n",
      "cdc\n",
      "dec\n",
      "dg\n",
      "formation\n",
      "gould\n",
      "harris\n",
      "honeywell\n",
      "hp\n",
      "ibm\n",
      "ipl\n",
      "magnuson\n",
      "nas\n",
      "ncr\n",
      "nixdorf\n",
      "perkin-elmer\n",
      "prime\n",
      "siemens\n",
      "sperry\n",
      "wang\n",
      "538.8099861488878 140 24\n"
     ]
    }
   ],
   "source": [
    "tree = expand_tree(tree_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, sample):\n",
    "    if(len(tree['childrens'])==0): #Leaf\n",
    "        data = fix_columns(pd.get_dummies(sample),pd.get_dummies(tree['samples']).columns)\n",
    "        print(data.shape)\n",
    "        return tree['model'].predict(data)\n",
    "    if tree['threshold']==None: #Categorical\n",
    "        found = False\n",
    "        for index, child in enumerate(tree['childrens']):\n",
    "            if sample[tree['split_by_feature']].values[0] == child['nominal_value']:\n",
    "                print(\"Found\")\n",
    "                node = child\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            print(\"Not found\")\n",
    "            return tree['model'].predict(fix_columns(pd.get_dummies(sample),pd.get_dummies(tree['samples']).columns))\n",
    "    else: #Numeric\n",
    "        if(sample[tree['split_by_feature']].values[0]>=tree['threshold']):\n",
    "            node = tree['childrens'][0] #right node\n",
    "        else:\n",
    "            node = tree['childrens'][1]\n",
    "    return predict(node, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n",
      "(1, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([35.99309084])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(tree, X_test.iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Not found\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Not found\n",
      "Not found\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Not found\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Not found\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 11)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 10)\n",
      "Found\n",
      "(1, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22632.07032736393"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [predict(tree,row.to_frame().T) for index,row in X_test.iterrows()]\n",
    "mean_squared_error(preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "train_dummed = pd.get_dummies(X_train)\n",
    "regressor.fit(train_dummed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113.3333333333333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = regressor.predict(fix_columns(pd.get_dummies(X_test), pd.get_dummies(X_train).columns))\n",
    "mean_squared_error(y_1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
